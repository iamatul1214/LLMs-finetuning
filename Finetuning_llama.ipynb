{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamatul1214/LLMs-finetuning/blob/main/Finetuning_llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T9M6xmaELqo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Transformers and LLMs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlBrPZB6Ec9H"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRrOuniTEe7v"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVK-Ur45vINK"
      },
      "source": [
        "## FineTuning With Unsloth\n",
        "#### In this tutorial we'll finetune the Llama-3.2-1B-Instruct model using unsloth on the ServiceNow-AI/R1-Distill-SFT dataset to empower the llama model with DeepSeek-R1 like 'thinking' capabilities.\n",
        "\n",
        "#### As we know the Llama-3.2-1B-Instruct model is itself a finetuned model from Llama-3.2-3B foundation model but it lacks reasoning with thinking capabilites, Let's finetune it to achieve that. We will use the dataset of ServiceNow-AI/R1-Distill-SFT which is created in such a way that we have thinking steps included in it for every row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfi08dfMuZwA"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seK5TMKIvsAC"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\", # or choose \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y-jw2J-wUcp"
      },
      "source": [
        "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
        "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
        "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
        "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
        "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]\n",
        "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]\n",
        "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]\n",
        "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]\n",
        "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]\n",
        "\n",
        "#### The parameters are\n",
        "- `model_name`:Specifies the name of the pre-trained model to load.\n",
        "\n",
        "- `max_seq_length`: Defines the maximum sequence length (in tokens) that the model can process. max_seq_length = 2048 allows the model to process sequences up to 2048 tokens long.\n",
        "\n",
        "- `dtype`: Specifies the data type for model weights and computations. None: Automatically selects the appropriate data type based on the hardware.\n",
        "torch.float16: Uses 16-bit floating point precision, reducing memory usage and potentially increasing speed on compatible GPUs. torch.bfloat16: Similar to float16 but with a wider dynamic range, beneficial for certain hardware like NVIDIA A100 GPUs.\n",
        "\n",
        "- `load_in_4bit`: Determines whether to load the model using 4-bit quantization.Ideal for scenarios where memory efficiency is crucial, such as deploying models on edge devices or during experimentation.\n",
        "Now, we'll use the get_peft_model from unsloth's FastLanguageModel class to attach adapters (peft layers) on top of the models in order to perform QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24UD9lDRLYFk"
      },
      "source": [
        "## Let's test the model's thinking and reasoning before finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfHGJ5oXLXg4"
      },
      "outputs": [],
      "source": [
        "# You can then perform inference with the reloaded model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\"\"\"\n",
        "\n",
        "message = sys_prompt.format(\"When son was 15 years old father was twice his age, what is the age of father when son becomes 30 years old\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
        "                                 temperature = 1.5, min_p = 0.1)\n",
        "reloaded_response = tokenizer.batch_decode(outputs)\n",
        "print(reloaded_response[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqmOI0Maw38M"
      },
      "source": [
        "### Now, we'll use the get_peft_model from unsloth's FastLanguageModel class to attach adapters (peft layers) on top of the models in order to perform QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEopvn6ZwK_l"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],  # These are the query,key,value vectors\n",
        "    lora_alpha = 16, # a higher alpha value assigns more weight to the LoRA activations\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDSoYawvxKlp"
      },
      "source": [
        "### Following are the hyperparameter definitions:\n",
        "\n",
        "- `r`: The rank of the low-rank matrices in LoRA; higher values can capture more information but increase memory usage.\n",
        "- `target_modules`: List of model components (e.g., \"q_proj\", \"k_proj\") where LoRA adapters are inserted for fine-tuning.\n",
        "- `lora_alpha`: Scaling factor for the LoRA updates; controls the impact of the adapters on the model's outputs.\n",
        "- `lora_dropout`: Dropout rate applied to LoRA layers during training to prevent overfitting.\n",
        "- `bias`: Specifies how biases are handled in LoRA layers; options include \"none\", \"all\", or \"lora_only\".\n",
        "- `use_gradient_checkpointing`: Enables gradient checkpointing to reduce memory usage during training; \"unsloth\" uses Unsloth's optimized version.\n",
        "- `random_state`: Seed for random number generators to ensure reproducibility of training results.\n",
        "- `use_rslora`: Boolean indicating whether to use Rank-Stabilized LoRA (rsLoRA) for potentially more stable training.\n",
        "- `loftq_config`: Configuration for Low-Rank Quantization (LoftQ); set to None to disable this feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9uXXaFSxF9b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0', split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw_fA2EGxGIn"
      },
      "outputs": [],
      "source": [
        "print(dataset[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghOJ9EC-xGQu"
      },
      "outputs": [],
      "source": [
        "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\n",
        "{}\n",
        "{}\n",
        "\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  problems = examples[\"problem\"]\n",
        "  thoughts = examples[\"reannotated_assistant_content\"]\n",
        "  solutions = examples[\"solution\"]\n",
        "  texts = []\n",
        "\n",
        "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
        "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTZ_u0mhyDf6"
      },
      "source": [
        "### Trainer Setup:\n",
        "\n",
        "- `model and tokenizer`: These are the model and tokenizer objects that will be trained.\n",
        "\n",
        "- `train_dataset`: The dataset used for training.\n",
        "\n",
        "- `dataset_text_field`: Specifies the field in the dataset that contains the text data.\n",
        "\n",
        "- `max_seq_length`: Maximum sequence length for the input data.\n",
        "\n",
        "- `dataset_num_proc`: Number of processes to use for data loading.\n",
        "\n",
        "- `packing`: If True, enables sequence packing (concatenates multiple examples into a single sequence to better utilize tokens).\n",
        "\n",
        "### Training Arguments:\n",
        "\n",
        "- `per_device_train_batch_size`: Number of samples per batch for each device.\n",
        "\n",
        "- `gradient_accumulation_steps`: Number of steps to accumulate gradients before updating model weights.\n",
        "\n",
        "- `warmup_steps`: Number of steps for learning rate warmup.\n",
        "\n",
        "- `max_steps`: Total number of training steps.\n",
        "\n",
        "- `learning_rate`: Learning rate for the optimizer.\n",
        "\n",
        "- `fp16 and bf16`: Specifies whether to use 16-bit floating point precision or bfloat16, depending on hardware support.\n",
        "\n",
        "- `logging_steps`: Frequency of logging training progress.\n",
        "\n",
        "- `optim`: Optimizer type, here using an 8-bit version of AdamW.\n",
        "\n",
        "- `weight_decay`: Regularization parameter for weight decay.\n",
        "\n",
        "- `lr_scheduler_type`: Type of learning rate scheduler.\n",
        "\n",
        "- `seed`: Random seed for reproducibility.\n",
        "\n",
        "- `output_dir`: Directory where the training outputs will be saved.\n",
        "\n",
        "- `report_to`: Integration for observability tools like \"wandb\", \"tensorboard\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehebCUdzxGZv"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # Number of processors to use for processing the dataset\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2, # The batch size per GPU/TPU core\n",
        "        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation\n",
        "        warmup_steps = 5, # Few updates with low learning rate before actual training\n",
        "        max_steps = 60, # Specifies the total number of training steps (batches) to run.\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\", # Optimizer\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc for observability\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsEr47aYxGiU"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XfUjLIOMiA"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\"\"\"\n",
        "message = sys_prompt.format(\"When son was 15 years old father was twice his age, what is the age of father when son becomes 30 years old\")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "response = tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-zXrscLxGrf"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\"\"\"\n",
        "message = sys_prompt.format(\"How many 'r's are present in 'strawberry'?\")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "response = tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Z4gK-WxG0Z"
      },
      "outputs": [],
      "source": [
        "print(response[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX69HX4g0EdN"
      },
      "source": [
        "## Local saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB6fjozT0D58"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"Atul_Llama-001-1B\")  # Local saving\n",
        "tokenizer.save_pretrained(\"Atul_Llama-001-1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qugVmOvLKkcm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpJrUrDCKuo1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qNm3BqKvHi"
      },
      "source": [
        "## Reload the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95fa60ee"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Reload the model and tokenizer from the saved directory\n",
        "reloaded_model, reloaded_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Atul_Llama-001-1B\", # Specify the path to your saved model\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# You can then perform inference with the reloaded model\n",
        "FastLanguageModel.for_inference(reloaded_model)\n",
        "\n",
        "message = sys_prompt.format(\"When son was 15 years old father was twice his age, what is the age of father when son becomes 30 years old\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "\n",
        "inputs = reloaded_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = reloaded_model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
        "                                 temperature = 1.5, min_p = 0.1)\n",
        "reloaded_response = reloaded_tokenizer.batch_decode(outputs)\n",
        "print(reloaded_response[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8VrKq0QLIbZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1esNk6PxCYJFgIXFHHs-J_XphOfbPVvQs",
      "authorship_tag": "ABX9TyMJebcOd17kamysY+F6nVXC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}